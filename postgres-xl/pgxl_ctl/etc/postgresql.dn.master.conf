# 8 core 64G SSD

#------------------------------------------------------------------------------
# BASE
#------------------------------------------------------------------------------

# 节点名字，后期修改
pgxc_node_name = 'datanode1'

datestyle = 'iso, mdy'
timezone = 'UTC'
lc_messages = 'en_US.UTF-8'
lc_monetary = 'en_US.UTF-8'
lc_numeric = 'en_US.UTF-8'
lc_time = 'en_US.UTF-8'
default_text_search_config = 'pg_catalog.english'

max_coordinators = 8
max_datanodes = 16


#------------------------------------------------------------------------------
# CONNECTION
#------------------------------------------------------------------------------

listen_addresses = '*'
port = 23001

max_connections = 8192
superuser_reserved_connections = 4

# 连接其他节点的连接池进程端口
pooler_port = 23101

# GTM
gtm_host = 'localhost'
gtm_port = 20002

# 避免网络层设备主动断开空闲连接的问题
tcp_keepalives_idle = 60
tcp_keepalives_interval = 10
tcp_keepalives_count = 10

# 貌似datanode之间的连接也需要配置连接池
max_pool_size = 256
pool_maintenance_timeout = -1
pool_conn_keepalive = 30


#------------------------------------------------------------------------------
# RESOURCE USAGE
#------------------------------------------------------------------------------

dynamic_shared_memory_type = posix

# 通常设置为25%的物理内存
shared_buffers = 16GB

# 用于session级临时表
temp_buffers = 8MB

# 最大预提交事务数，通常与max_connections相同，保证每个连接都可以用
max_prepared_transactions = 8192

# 用于ORDER BY, DISTINCT, merge joins, hash joins, hash-based aggregation, hash-based processing of IN subqueries的单进程最大内存
work_mem = 4MB

# 用于VACUUM, CREATE INDEX, ALTER TABLE ADD FOREIGN KEY的单进程最大内存。对于单机PostgreSQL可以在执行操作前通过SET maintenance_work_mem = '4GB'来改变当前会话的设定，但是对于PGXL仅生效于Coordinator，而不影响Datanode
maintenance_work_mem = 2GB

# 用于每个autovacuum进程的最大内存，如果没有设置此参数则使用maintenance_work_mem的设定。当数据库执行autovacuum时最多占用内存为 autovacuum_work_mem * autovacuum_max_workers
autovacuum_work_mem = 1GB

# 用于wal(write ahead log)缓冲区的内存，通常设置为3%的shared_buffers
wal_buffers = 512MB

# 缓存大小，减掉数据库shared buffer, maintenance work mem, autovacuum work mem, work mem, wal buffer的剩余
effective_cache_size = 22GB

# PGXL的用于datanode之间数据交换的东西，未来这部分会被重新设计，尽量避免使用子查询带来datanode数据交换
shared_queues = 8192
shared_queue_size = 64kB

max_stack_depth = 2MB

max_files_per_process = 1024

max_worker_processes = 16


effective_io_concurrency = 0


# 设置bgwriter(Background Writer)快速刷dirty(new or modified)shared buffers。较小的bgwriter_lru_maxpages和bgwriter_lru_multiplier可以减少IO，但是可能造成进程自己处理写入，增大交互查询延迟
bgwriter_delay = 10ms
bgwriter_lru_maxpages = 1000
bgwriter_lru_multiplier = 10.0

random_page_cost = 1.3


#------------------------------------------------------------------------------
# VACCUM
#------------------------------------------------------------------------------

autovacuum_max_workers = 4

autovacuum_naptime = 20s

# 垃圾版本超过5%时，触发自动垃圾回收
autovacuum_vacuum_scale_factor = 0.05
# 表的年龄超过16亿时，强制触发vacuum freeze(即使没有开启autovacuum)
autovacuum_freeze_max_age = 1600000000

# 年龄超过5000万的记录，都设置为冻结年龄
vacuum_freeze_min_age = 50000000    
vacuum_multixact_freeze_min_age = 5000000       

# 表的年龄超过12亿时，autovacuum 触发vacuum freeze
vacuum_freeze_table_age = 1200000000
vacuum_multixact_freeze_table_age = 1100000000

# SSD, 建议设置为0, 不打断vacuum 
vacuum_cost_delay = 0

#------------------------------------------------------------------------------
# LOG
#------------------------------------------------------------------------------

log_timezone = 'UTC'
log_destination = 'stderr'
logging_collector = on
client_min_messages = notice
log_min_messages = warning
#log_min_messages = debug1
log_min_error_statement = error
log_error_verbosity = default
#log_error_verbosity = verbose
log_statement = 'none'

# 日志目录，后期修改
log_directory = '/mfplog/pgxl_log/datanode'
log_filename = 'dn.%Y-%m-%d.log'
log_line_prefix = '%m %p %x'
log_file_mode = 0600
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 1GB

# 打印超过x毫秒的会话
log_min_duration_statement = 200
#log_min_duration_statement = 100
# 打印超过x毫秒的autovacuum
log_autovacuum_min_duration = 0

debug_print_parse = off
debug_print_rewritten = off
debug_print_plan = off
debug_pretty_print = on
log_checkpoints = on
log_connections = off
log_disconnections = off
log_duration = off
log_hostname = off
log_lock_waits = on
log_replication_commands = on


#------------------------------------------------------------------------------
# STATISTICS
#------------------------------------------------------------------------------

track_activities = on
track_counts = on
track_io_timing = on
track_functions = 'pl'

shared_preload_libraries = 'pg_stat_statements'


# 最多保留的统计信息数
pg_stat_statements.max = 10000
# all: 所有SQL包括函数内嵌套的SQL; top: 直接执行的SQL; none: 不跟踪    
pg_stat_statements.track = all
# 是否跟踪非DML语句
pg_stat_statements.track_utility = off
# 重启后是否保留统计信息
pg_stat_statements.save = on


# 启用 auto_explain 会拉低数据库性能
#shared_preload_libraries = 'pg_stat_statements,auto_explain'

# 为了方便查看,这里把时间设置为0,所有SQL都会被auto_explain捕获输出.实际使用的时候适当调大。如 100ms
#auto_explain.log_min_duration = 0
#auto_explain.log_analyze = true
#auto_explain.log_verbose = true
#auto_explain.log_buffers = true
#auto_explain.log_nested_statements = true


#------------------------------------------------------------------------------
# WRITE AHEAD LOG (WAL) AND REPLICATION
#------------------------------------------------------------------------------

# hot_standby用于standby节点可提供只读服务，但是PGXL的standby不支持从GTM获取xid，所以不能提供只读
#wal_level = hot_standby
wal_level = archive
archive_mode = on
# 归档命令，后期修改
archive_command = 'rsync %p postgres@pgxl-dn-s-1:/mfpdata/pgxl_data/datanode/pg_alog/%f'

max_wal_senders = 8

checkpoint_timeout = 30min
# 硬盘好的情况下，可以让检查点快速结束，恢复时也可以快速达到一致状态。
checkpoint_completion_target = 0.2
checkpoint_warning = 30s

# 通常为shared_buffers的2倍
max_wal_size = 16GB
min_wal_size = 1GB

wal_keep_segments = 1024
wal_sender_timeout = 30s
wal_writer_delay = 10ms

# 设置事务提交是否等待WAL写到磁盘
synchronous_commit = on

# 不要设置这个
####synchronous_standby_names = 'datanode1'


#------------------------------------------------------------------------------
# Others
#------------------------------------------------------------------------------

#deadlock_timeout = 5s
#global_snapshot_source = 'coordinator'


